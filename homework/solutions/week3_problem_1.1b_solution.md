# Week 3 Homework - Problem 1.1-(b) Solution
## Total Parameters in Multi-Head Attention Block

---

## Problem Statement

Calculate the **TOTAL number of parameters** in the entire Multi-Head Attention (MHA) block, including:
- Q, K, V projections (from part a)
- Output projection W_O
- All bias terms

**Given:**
- Model dimension: d_model = 512
- Number of attention heads: h = 8
- Head dimension: d_k = d_v = 64
- All linear layers include bias

---

## Step 1: Recap from Problem 1.1-(a)

### Q, K, V Projections

In part (a), we calculated parameters for the Query, Key, and Value projections.

**For EACH projection (Q, K, or V):**

```
Weight matrix: W_Q âˆˆ â„^(d_model Ã— d_model)
Bias vector:   b_Q âˆˆ â„^(d_model)

Parameters per projection = d_model Ã— d_model + d_model
                          = 512 Ã— 512 + 512
                          = 262,144 + 512
                          = 262,656 parameters
```

**For all THREE projections (Q, K, V):**

```
Total Q,K,V parameters = 3 Ã— 262,656
                       = 787,968 parameters
```

### Why these dimensions?

Each projection transforms the input from d_model dimensions to d_model dimensions:
- Input: [batch_size, seq_len, d_model=512]
- Output: [batch_size, seq_len, d_model=512]

Then internally, the attention mechanism splits this into h=8 heads of dimension d_k=64 each (512 = 8 Ã— 64).

---

## Step 2: Understanding the Output Projection (W_O)

### What is W_O?

After multi-head attention computes attention across all heads:

1. **Each head produces:** [batch_size, seq_len, d_k=64]
2. **Concatenate all h heads:** [batch_size, seq_len, h Ã— d_k] = [batch_size, seq_len, 512]
3. **Apply output projection W_O:** Transform back to [batch_size, seq_len, d_model=512]

### Why do we need W_O?

The output projection serves several crucial purposes:

1. **Dimension transformation:** Projects the concatenated heads (d_model dimensions) back to d_model
2. **Information mixing:** Allows the model to learn how to combine information from different heads
3. **Added expressiveness:** Provides additional learnable parameters to refine the attention output
4. **Architectural consistency:** Ensures the output matches the input dimension for residual connections

### Mathematical Flow

```
Head_i = Attention(Q_i, K_i, V_i)  âˆˆ â„^(seq_len Ã— d_k)  for i = 1,...,h

Concat = [Head_1; Head_2; ...; Head_h]  âˆˆ â„^(seq_len Ã— (h Ã— d_k))
                                        = â„^(seq_len Ã— d_model)

Output = Concat Â· W_O + b_O  âˆˆ â„^(seq_len Ã— d_model)
```

---

## Step 3: Calculate Parameters for Output Projection (W_O)

### W_O Weight Matrix

The output projection transforms from d_model to d_model:

```
W_O âˆˆ â„^(d_model Ã— d_model)

Parameters in W_O = d_model Ã— d_model
                  = 512 Ã— 512
                  = 262,144 parameters
```

### b_O Bias Vector

```
b_O âˆˆ â„^(d_model)

Parameters in b_O = d_model
                  = 512 parameters
```

### Total for Output Projection

```
Total W_O parameters = 262,144 + 512
                     = 262,656 parameters
```

**Note:** This is exactly the same as ONE of the Q, K, or V projections!

---

## Step 4: Calculate TOTAL Parameters for Entire MHA Block

### Summary of All Components

| Component | Weight Parameters | Bias Parameters | Total |
|-----------|------------------|-----------------|-------|
| Q projection (W_Q) | 512 Ã— 512 = 262,144 | 512 | 262,656 |
| K projection (W_K) | 512 Ã— 512 = 262,144 | 512 | 262,656 |
| V projection (W_V) | 512 Ã— 512 = 262,144 | 512 | 262,656 |
| Output projection (W_O) | 512 Ã— 512 = 262,144 | 512 | 262,656 |
| **TOTAL** | **1,048,576** | **2,048** | **1,050,624** |

### Step-by-Step Calculation

```
Method 1: Component-by-component addition
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Q projection:     262,656
K projection:     262,656
V projection:     262,656
O projection:     262,656
                  â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL:          1,050,624 parameters


Method 2: Formula-based calculation
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Each projection has: d_modelÂ² + d_model parameters
Number of projections: 4 (Q, K, V, O)

TOTAL = 4 Ã— (d_modelÂ² + d_model)
      = 4 Ã— (512Â² + 512)
      = 4 Ã— (262,144 + 512)
      = 4 Ã— 262,656
      = 1,050,624 parameters
```

---

## Step 5: Verification and Insights

### Verification Check âœ“

```
Weight parameters: 4 Ã— 512Â² = 4 Ã— 262,144 = 1,048,576 âœ“
Bias parameters:   4 Ã— 512  = 2,048 âœ“
Total:             1,048,576 + 2,048 = 1,050,624 âœ“
```

### Key Insights

1. **Symmetry:** Each of the four projections (Q, K, V, O) has the same number of parameters (262,656)

2. **Dominant factor:** The weight matrices (d_modelÂ²) dominate the parameter count:
   - Weights: 1,048,576 (99.8%)
   - Biases: 2,048 (0.2%)

3. **Scaling:** Parameters scale **quadratically** with d_model:
   - If d_model doubles â†’ parameters increase by 4Ã—
   - Example: d_model=1024 would have 4Ã— as many parameters

4. **Head independence:** Interestingly, the number of heads (h=8) does NOT affect the total parameter count! The heads share the same projection matrices.

---

## Final Answer

**The total number of parameters in the Multi-Head Attention block is:**

# **1,050,624 parameters**

### Breakdown:
- **Q, K, V projections:** 787,968 parameters (from part a)
- **Output projection (W_O):** 262,656 parameters
- **Total:** 1,050,624 parameters

### Alternative expression:
```
TOTAL = 4 Ã— (d_modelÂ² + d_model)
      = 4d_modelÂ² + 4d_model
      = 4d_model(d_model + 1)
      = 4 Ã— 512 Ã— 513
      = 1,050,624 parameters
```

---

## Educational Notes

### Why is this calculation important?

1. **Model size estimation:** Helps understand memory requirements
2. **Computational cost:** Parameter count correlates with computation
3. **Comparison:** Enables comparison with other architectures
4. **Optimization:** Identifies where parameter reduction might help

### Common mistakes to avoid:

âŒ **Forgetting the output projection** - W_O is essential but easy to miss
âŒ **Forgetting bias terms** - Each projection has both weights and biases
âŒ **Incorrect dimensions** - All projections are d_model Ã— d_model, not d_model Ã— (h Ã— d_k)
âŒ **Confusing head count** - Number of heads doesn't change parameter count

### Comparison to other layers:

For context, a typical Transformer also has:
- **Feed-forward network:** ~2M parameters (with d_ff = 2048)
- **Layer normalization:** ~1K parameters
- The MHA block (~1M params) is a significant portion of each layer!

---

## Appendix: General Formula

For any Multi-Head Attention block:

```
Total_Parameters = 4 Ã— (d_modelÂ² + d_model)

Where:
- d_model: model dimension
- 4: number of projections (Q, K, V, O)
- Each projection: weight matrix (d_modelÂ²) + bias (d_model)
```

**Note:** This formula is independent of:
- Number of heads (h)
- Sequence length
- Batch size

---

**Solution complete!** ğŸ“
